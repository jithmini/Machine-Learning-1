{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error Tunnel connection\n",
      "[nltk_data]     failed: 502 Fiddler - DNS Lookup Failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error Tunnel connection\n",
      "[nltk_data]     failed: 502 Fiddler - DNS Lookup Failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error Tunnel connection\n",
      "[nltk_data]     failed: 502 Fiddler - DNS Lookup Failed>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer, TreebankWordTokenizer\n",
    "from nltk.stem.snowball import  EnglishStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stop_words = set(stopwords.words('english') + list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer() \n",
    "detokenizer = TreebankWordDetokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = EnglishStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_word_list = set([word for word in wordnet.words(lang='eng')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(dir):\n",
    "    allfiles = []\n",
    "    for roots, dirs, files in os.walk(dir):\n",
    "        for file in files:\n",
    "            allfiles.append(os.path.join(roots, file))\n",
    "    return allfiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_loca(file_paths):\n",
    "    comp = []\n",
    "    rec = []\n",
    "    sci = []\n",
    "    talk = []\n",
    "    \n",
    "    for path in file_paths:\n",
    "        if \"comp\" in path:\n",
    "            w = open(path, encoding='utf-8', errors='ignore')\n",
    "            comp += [w.read()]\n",
    "        elif \"rec\" in path:\n",
    "            w = open(path, encoding='utf-8', errors='ignore')\n",
    "            rec += [w.read()]\n",
    "        elif \"sci\" in path:\n",
    "            w = open(path, encoding='utf-8', errors='ignore')\n",
    "            sci += [w.read()]\n",
    "        elif \"talk\" in path:\n",
    "            w = open(path, encoding='utf-8', errors='ignore')\n",
    "            talk += [w.read()]\n",
    "    return comp, rec, sci, talk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_unique_words(data_list):\n",
    "    word_set = []\n",
    "    for docs in data_list:\n",
    "        for doc in docs:\n",
    "            words = []\n",
    "            for word in word_tokenize(doc):\n",
    "                word = stemmer.stem(word)\n",
    "                word = lemmatizer.lemmatize(word)\n",
    "                words += [word]\n",
    "            word_set += [word for word in words if word not in _stop_words if word in _word_list if word.isalpha()]\n",
    "    return list(set(word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    cleaned_data = []\n",
    "    for text in data:\n",
    "        words = []\n",
    "        for word in tokenizer.tokenize(text):\n",
    "            word = stemmer.stem(word)\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            words += [word]   \n",
    "        cleaned_data += [detokenizer.detokenize([word for word in words if word in _word_list if word.isalpha()])]\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vectorize_csv(data, name, vocabulary_set):\n",
    "    cleaned_data = clean_data(data)\n",
    "    vectorizer = CountVectorizer(stop_words=_stop_words, vocabulary=vocabulary_set)\n",
    "    x = vectorizer.fit_transform(cleaned_data)\n",
    "    y = vectorizer.get_feature_names()\n",
    "    \n",
    "    df = pd.DataFrame(data=x.toarray(), columns=y)\n",
    "    \n",
    "    df.to_csv(name+'.csv')\n",
    "    \n",
    "    print(name+'.csv successfully created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "files = read_files('./')\n",
    "\n",
    "comp_data = []\n",
    "rec_data = []\n",
    "sci_data = []\n",
    "talk_data = []\n",
    "\n",
    "comp_data, rec_data, sci_data, talk_data = file_loca(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_train, comp_test = train_test_split(comp_data, train_size=0.7, test_size=0.3, random_state=42)\n",
    "rec_train, rec_test = train_test_split(rec_data, train_size=0.7, test_size=0.3, random_state=42)\n",
    "sci_train, sci_test = train_test_split(sci_data, train_size=0.7, test_size=0.3, random_state=42)\n",
    "talk_train, talk_test = train_test_split(talk_data, train_size=0.7, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [comp_train, rec_train, sci_train, talk_train]\n",
    "test_data = [comp_test, rec_test, sci_test, talk_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_diction = data_unique_words(train_data)\n",
    "data_diction.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_vectorize_csv(comp_train, 'computer_train', data_diction)\n",
    "make_vectorize_csv(comp_test, 'computer_test', data_diction)\n",
    "\n",
    "make_vectorize_csv(rec_train, 'recreational_train', data_diction)\n",
    "make_vectorize_csv(rec_test, 'recreational_test', data_diction)\n",
    "\n",
    "make_vectorize_csv(sci_train, 'science_train', data_diction)\n",
    "make_vectorize_csv(sci_test, 'science_test', data_diction)\n",
    "\n",
    "make_vectorize_csv(talk_train, 'talk_train', data_diction)\n",
    "make_vectorize_csv(talk_test, 'talk_test', data_diction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
